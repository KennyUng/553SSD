{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_nn",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRYpISJh6uzr"
      },
      "source": [
        "# Setting up the dataset\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# Downloading the dataset from the link\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM6t2v9Q7Elu"
      },
      "source": [
        "# unpacking the data by unpickling\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUbugD2w7UNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e60317c-9dcc-4864-a9fd-2f837105c352"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Transforming the x and y datasets into tensors for Pytorch usage\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "\n",
        "n, c = x_train.shape\n",
        "print(x_train, y_train)\n",
        "print(x_train.shape)\n",
        "print(y_train.min(), y_train.max())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "torch.Size([50000, 784])\n",
            "tensor(0) tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-QmWy3V7URq"
      },
      "source": [
        "# Training a neural net without the nn module\n",
        "import math\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_() # We can use requires_grad_() to calculate gradient automatically on the weights\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuT1hTfM8Vwa"
      },
      "source": [
        "# We need an activation function\n",
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "# We can return any function as a model because pytorch has automatic gradient calculation\n",
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias) # @ symbol is a dot product"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_LgHw8o8oqj",
        "outputId": "efb2b922-e61b-4d09-b99a-26a2e568eba7"
      },
      "source": [
        "# Create one forward pass\n",
        "bs = 64\n",
        "xb = x_train[0:bs] # 1 minibatch\n",
        "preds = model(xb)\n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.6538, -2.4375, -2.5406, -2.0018, -2.1615, -1.7220, -2.7385, -2.6682,\n",
            "        -2.0823, -2.5799], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwnd29v-9BBG"
      },
      "source": [
        "# Create a loss function to do backpropagation\n",
        "def nll(input, target):\n",
        "  return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs9GNvqE9QdE",
        "outputId": "885eb61a-9714-46df-df2e-7272c7899536"
      },
      "source": [
        "# Calculate the loss on the predictions vs the ground truth\n",
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds, yb))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3777, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IxHfZXH9lyW"
      },
      "source": [
        "# Calculate the accuracy of our predictions\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hfI6cuZ9qEq",
        "outputId": "78860216-71a8-4062-e067-0cb46a17bce7"
      },
      "source": [
        "print(accuracy(preds, yb))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0781)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJwDGTPW9y_f"
      },
      "source": [
        "# Basic training\n",
        "\n",
        "lr = 0.5\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i] # get the next minibatch\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model(xb) # predict the values using the model\n",
        "    loss = loss_func(pred, yb) # calculate the loss from the predictions vs ground truth\n",
        "\n",
        "    loss.backward() # perform backprop\n",
        "    with torch.no_grad():\n",
        "      weights -= weights.grad * lr # adjust the weights\n",
        "      bias -= bias.grad * lr # adjust the biases\n",
        "      weights.grad.zero_() # zero the gradients for the next minibatch\n",
        "      bias.grad.zero_()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WYHyqiY-wC8",
        "outputId": "1a1d519d-1a72-4589-a18b-210673d253f3"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0814, grad_fn=<NegBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47LHdVuf-yG3"
      },
      "source": [
        "# Implementing loss function with nn.functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "  return xb @ weights + bias"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD9EkGD2_uip",
        "outputId": "e025dd29-68b1-48be-a970-8066892e50bb"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0814, grad_fn=<NllLossBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ltsWezT_wuf"
      },
      "source": [
        "# Implementing the weights and biases using built in nn.Module functionality\n",
        "from torch import nn\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() # call super to subclass our module from nn.Module\n",
        "    self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) # implement weights into the model\n",
        "    self.bias = nn.Parameter(torch.zeros(10)) # implement biases into the model\n",
        "\n",
        "  # define what a single forward pass will look like in our model\n",
        "  def forward(self, xb):\n",
        "    return xb @ self.weights + self.bias"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFYbQhvWAhJt"
      },
      "source": [
        "model = Mnist_Logistic()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1r_noubAjjl",
        "outputId": "8fd8e3d1-105e-4d94-83cd-da16f68f0b93"
      },
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4315, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQTqSpnA5gl"
      },
      "source": [
        "# Redefine our training loop into a function\n",
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n - 1) // bs + 1):\n",
        "            start_i = i * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # We can simplify our training loop so that we update all of our parameters at once\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad() # We can also zero out the gradient for the entire model with one command via nn module"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOZw5y9MBKAu",
        "outputId": "414b0e33-4c31-4010-91ff-87af1297d497"
      },
      "source": [
        "fit()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0831, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9y-6rlaBotb"
      },
      "source": [
        "# Refactor our model using Linear layers\n",
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lin = nn.Linear(784, 10) # Define a linear layer, which contains the weights and biases\n",
        "  \n",
        "  def forward(self, xb):\n",
        "    return self.lin(xb)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coj4ZOC3B_hn",
        "outputId": "88e3ff2d-c8d1-4ae9-8fe0-4eb7128cd096"
      },
      "source": [
        "# Check that everything works the same as before\n",
        "model = Mnist_Logistic()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3576, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyX7ZTPrCCDn",
        "outputId": "5fd6f613-f54b-4ef8-9e9a-b851a33ea4d2"
      },
      "source": [
        "fit()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0818, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWQZ42KcCG7h"
      },
      "source": [
        "# We can further improve our training function by using torch.optim\n",
        "from torch import optim"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-y_duWaCaos"
      },
      "source": [
        "# We can get the model and optimizer in one function\n",
        "def get_model():\n",
        "  model = Mnist_Logistic()\n",
        "  return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "model, opt = get_model()\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmAqEW8pC_kD",
        "outputId": "3a535924-df17-4018-d6af-e7ddcb8987e9"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step() # Call the optimizer to update our parameters via the forward step function\n",
        "        opt.zero_grad() # Zero out the gradients for the next minibatch\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0797, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5aG6S-RDPcf"
      },
      "source": [
        "# Learning how to use Dataset\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train) # combining them into a single tensor makes it easier to iterate over"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oVIQ3RuDtIy",
        "outputId": "b0bb9eb8-3436-4806-c91d-020f2b1b6cb7"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        xb, yb = train_ds[i * bs: i * bs + bs] # We can get x batch and y batch in one statement\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0815, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyDSWOCBElAS"
      },
      "source": [
        "# Further improvements to minibatch via dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs) # Batches are generated automatically"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYpEC4LtE3AO",
        "outputId": "812b90a5-620c-4aa5-c75b-eb30484ff952"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dl: # train_dl will generate the minibatches automatically\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0813, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOkOPOC9FBvP"
      },
      "source": [
        "# Adding validation to our training\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2) # Validation batches are twice as big since we don't need to backprop them"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNfoQwBLFbe4",
        "outputId": "3c14755c-a480-4db1-c79f-5351e4ff14b2"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # We indicate this is the training stage (use the train set)\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    model.eval() # Indicate this is the eval stage (use the validation set)\n",
        "    # We do this because some layers in neural networks behave differently during training vs eval\n",
        "    with torch.no_grad():\n",
        "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
        "\n",
        "    print(epoch, valid_loss / len(valid_dl))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.4322)\n",
            "1 tensor(0.2791)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIWGKhwdF7Rz"
      },
      "source": [
        "# Make Calculating loss into it's own function\n",
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "  loss = loss_func(model(xb), yb)\n",
        "\n",
        "  if opt is not None: # If there is an optimizer then perform backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "  \n",
        "  return loss.item(), len(xb)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbdFAhGNGMsC"
      },
      "source": [
        "# Define the fit function for training/validation\n",
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "      loss_batch(model, loss_func, xb, yb, opt) # we include opt so backprop is performed\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      losses, nums = zip(\n",
        "          *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl] # no opt is included here\n",
        "      )\n",
        "    val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "    print(epoch, val_loss)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzkVhaLGubn"
      },
      "source": [
        "# Generate the train and validation batches\n",
        "# We shuffle training batch so that there isn't overfitting based on batch familiarity\n",
        "def get_data(train_ds, valid_ds, bs):\n",
        "  return (\n",
        "      DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "      DataLoader(valid_ds, batch_size=bs * 2),\n",
        "  )"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWf896v-HL2H",
        "outputId": "76e3929b-c0fc-434b-aaa7-a8ca9d69c374"
      },
      "source": [
        "# We can use these three lines to perform our training\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "model, opt = get_model()\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.33284156425595285\n",
            "1 0.2883918229401112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OULEhqkFHZuo"
      },
      "source": [
        "# Switching our model to a CNN\n",
        "class Mnist_CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1) # Pytorch has a predefined conv2d layer\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=10, kernel_size=3, stride=2, padding=1)\n",
        "  \n",
        "  def forward(self, xb):\n",
        "    xb = xb.view(-1, 1, 28, 28) # Reshape the batch dimensions to 28 x 28\n",
        "    xb = F.relu(self.conv1(xb)) # perform relu after each cnn layer\n",
        "    xb = F.relu(self.conv2(xb))\n",
        "    xb = F.relu(self.conv3(xb))\n",
        "    xb = F.avg_pool2d(xb, 4) # End the forward pass with an average pooling\n",
        "    return xb.view(-1, xb.size(1))\n",
        "\n",
        "lr = 0.1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3iL_vk4Il-0",
        "outputId": "4857a70f-634c-4c39-9098-f129e0ebb6f9"
      },
      "source": [
        "model = Mnist_CNN()\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) # include momentum into our optimizer to improve training\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.315023463845253\n",
            "1 0.24123036992549896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C50F_q5rI0v8"
      },
      "source": [
        "# Implement sequential to improve the model implementation\n",
        "\n",
        "# We need to create a custom layer that will reshape our tensor first, since view is not a layer\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "  def __init__(self, func):\n",
        "    super().__init__()\n",
        "    self.func = func\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.func(x)\n",
        "  \n",
        "def preprocess(x):\n",
        "  return x.view(-1, 1, 28, 28)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kPfCP5JJdcP",
        "outputId": "90606edd-2db9-42e4-f3c9-682fde5c52d6"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.31321035504341127\n",
            "1 0.2552851420402527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Teica6wFJuxL"
      },
      "source": [
        "# Creating a dataloader that works with any sized image\n",
        "\n",
        "def preprocess(x, y):\n",
        "  return x.view(-1, 1, 28, 28), y\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "      "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX7nqrD9KNEk"
      },
      "source": [
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess) # Preprocess our batches to be the correct size\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErXaw-4sKiIB"
      },
      "source": [
        "# Use adaptive AvgPool2d so that the output tensor can be custom size\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), # note that the initial preprocess is gone\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1), \n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROtEFHXWKyy9",
        "outputId": "facc8170-9217-4c66-c8c0-a90fe5dc0ce3"
      },
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.3511363983631134\n",
            "1 0.23028322924375533\n"
          ]
        }
      ]
    }
  ]
}