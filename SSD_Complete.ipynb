{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD_Complete",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkfU7jSlzADW"
      },
      "source": [
        "<h1>DATASET LOADING</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XFyUtlBzbB_"
      },
      "source": [
        "#imports\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfeK9dvzzNjl",
        "outputId": "8beb2dab-3c43-4fca-ddd1-80323fb9b910"
      },
      "source": [
        "# get dataset from drive\n",
        "!gdown https://drive.google.com/uc?id=1Fjmt_xFGWKIHl8J7z8eIGhopobm7VzF1\n",
        "\n",
        "# unzip dataset\n",
        "!unzip -q /content/images_folder.zip\n",
        "\n",
        "directory = 'images_folder/'\n",
        "images= []\n",
        "\n",
        "folder = os.path.join(directory)\n",
        "for file in os.listdir(folder):\n",
        "  images.append(directory + file)\n",
        "\n",
        "images = np.sort(images)\n",
        "img = []\n",
        "for x in images:\n",
        "  img.append(Image.open(x))\n",
        "\n",
        "# download annotation text file from drive\n",
        "!gdown https://drive.google.com/uc?id=1FS21e-deAla84h6AWPBcMU0BPV-6sNIf\n",
        "\n",
        "# store annotations in an array\n",
        "file = '/content/annotations.txt'\n",
        "\n",
        "with open(file) as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "# take labels and assign numerical values \n",
        "labels = ('background', 'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'aeroplane', 'bicycle',\n",
        "          'boat', 'bus', 'car', 'motorbike', 'train', 'bottle', 'chair', 'diningtable', \n",
        "          'pottedplant', 'sofa', 'tvmonitor')\n",
        "\n",
        "labels_map = {k:v for v,k in enumerate(labels)}\n",
        "print(labels_map)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Fjmt_xFGWKIHl8J7z8eIGhopobm7VzF1\n",
            "To: /content/images_folder.zip\n",
            "100% 1.34G/1.34G [00:05<00:00, 238MB/s]\n",
            "replace images_folder/2008_000002.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FS21e-deAla84h6AWPBcMU0BPV-6sNIf\n",
            "To: /content/annotations.txt\n",
            "100% 4.13M/4.13M [00:00<00:00, 130MB/s]\n",
            "{'background': 0, 'person': 1, 'bird': 2, 'cat': 3, 'cow': 4, 'dog': 5, 'horse': 6, 'sheep': 7, 'aeroplane': 8, 'bicycle': 9, 'boat': 10, 'bus': 11, 'car': 12, 'motorbike': 13, 'train': 14, 'bottle': 15, 'chair': 16, 'diningtable': 17, 'pottedplant': 18, 'sofa': 19, 'tvmonitor': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sUXfO6Ozy8l"
      },
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "# parse annotations for image index\n",
        "def parseAnnotations(idx):\n",
        "  id = []\n",
        "  labels = []\n",
        "  bboxes = []\n",
        "  numObj = []\n",
        "  line = lines[idx]\n",
        "  split = line.strip().split()\n",
        "  id.append(split[0])\n",
        "\n",
        "  numOfObj = int(split[1])\n",
        "  numObj.append(numOfObj)\n",
        "  for i in range(numOfObj):\n",
        "    xmin = split[2+5*i]\n",
        "    ymin = split[3+5*i]\n",
        "    xmax = split[4+5*i]\n",
        "    ymax = split[5+5*i]\n",
        "    bboxes.append([float(xmin), float(ymin), float(xmax), float(ymax)])\n",
        "    labels.append(labels_map[split[6+5*i]])\n",
        "\n",
        "  tensorLabels = torch.LongTensor(labels)\n",
        "  image = img[idx]\n",
        "  resized_image = img[idx].resize((300,300))\n",
        "  width, height = resized_image.size\n",
        "  resized_image = transform(resized_image)\n",
        "  tensorBoxes = torch.Tensor(bboxes)\n",
        "  tensorBoxes /= torch.Tensor([width, height, width, height]).expand_as(tensorBoxes)\n",
        "  return resized_image, id, tensorLabels, tensorBoxes, numObj #{'id': id, 'label': labels, 'bboxes': torch.Tensor(bboxes), 'numObj': numObj}\n",
        "\n",
        "# get resized image, bboxes\n",
        "# def getImage(idx):\n",
        "#   image = img[idx]\n",
        "#   id, labels, bboxes, numObj = parseAnnotations(idx)\n",
        "#   # Scale the bounding boxes with 300x300 image\n",
        "#   width, height = image.size\n",
        "#   bboxes /= torch.Tensor([width, height, width, height]).expand_as(bboxes)\n",
        "\n",
        "#   resized_image = img[idx].resize((300,300))\n",
        "#   resized_image = transform(resized_image)\n",
        "#   return resized_image, id, labels, bboxes, numObj\n",
        "\n",
        "def create_dataset(image_list):\n",
        "  dataset = []\n",
        "  # validation_data = resized_image[math.floor(total*.60):math.floor(total*.80)]\n",
        "  # test_data = resized_image[math.floor(total*.80):total]\n",
        "  for i in range(len(image_list)):\n",
        "    dataset.append(parseAnnotations(i))\n",
        "  return dataset\n",
        "\n",
        "def split_dataset(dataset):\n",
        "  training_data = []\n",
        "  validation_data = []\n",
        "  test_data = []\n",
        "\n",
        "  random.shuffle(dataset)\n",
        "  total = len(dataset)\n",
        "\n",
        "  # split data into 60/20/20 of training, validation, test\n",
        "  training_data = dataset[0:math.floor(total*.60)]\n",
        "  validation_data = dataset[math.floor(total*.60):math.floor(total*.80)]\n",
        "  test_data = dataset[math.floor(total*.80):total]\n",
        "  # for i in range(len(training_data)):\n",
        "  #   image,id,label,bboxes,numObj = getImage(i) \n",
        "  #   training_data[i] = data_augmentation(image, bboxes)\n",
        "\n",
        "  return training_data, validation_data, test_data\n",
        "\n",
        "def data_augmentation(image, bboxes):\n",
        "  randNum = random.random()\n",
        "  w = 300\n",
        "  h = 300\n",
        "  if randNum < 0.5:\n",
        "    image = torch.flip(image, [0,1])\n",
        "    xmin = w - (bboxes[:,2] * 300)\n",
        "    ymin = h - (bboxes[:,3] * 300)\n",
        "    xmax = w - (bboxes[:,0] * 300)\n",
        "    ymax = h - (bboxes[:,1] * 300)\n",
        "\n",
        "    bboxes[:,0] = xmin / 300\n",
        "    bboxes[:,1] = ymin / 300\n",
        "    bboxes[:,2] = xmax / 300\n",
        "    bboxes[:,3] = ymax / 300\n",
        "  return image, bboxes\n",
        "\n",
        "# print(img)\n",
        "dataset = create_dataset(img)\n",
        "train_dataset, val_dataset, test_dataser = split_dataset(dataset)\n",
        "\n",
        "# train, val, test = split_dataset(dataset)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RDBoO0J0FVG"
      },
      "source": [
        "<h1>UTILS</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQo38BBPz7Vs"
      },
      "source": [
        "# Compute all default boxes\n",
        "def default_boxes():\n",
        "  scale = 300\n",
        "  # scaling and sizing of boxes\n",
        "  steps = [s / scale for s in (8, 16, 32, 64, 128, 256)]\n",
        "  sizes = [s / scale for s in (30, 60, 102, 144, 186, 228, 270)]\n",
        "  # aspect_ratios = ((1,), (2,), (3,), (1/2,), (1/3,))\n",
        "  aspect_ratios = ((2,), (2,3), (2,3), (2,3), (2,), (2,))\n",
        "  feature_map_dim = (38, 19, 10, 5, 3, 1)\n",
        "\n",
        "  num_layers = len(feature_map_dim)\n",
        "  boxes = []\n",
        "  # create priors(default boxes) for each layer\n",
        "  for i in range(num_layers):\n",
        "    fmsize = feature_map_dim[i]\n",
        "    for h,w in itertools.product(range(fmsize), repeat=2):\n",
        "      cx = (w + 0.5)*steps[i]\n",
        "      cy = (h + 0.5)*steps[i]\n",
        "\n",
        "      s = sizes[i]\n",
        "      boxes.append((cx, cy, s, s))\n",
        "\n",
        "      s = math.sqrt(sizes[i] * sizes[i+1])\n",
        "      boxes.append((cx, cy, s, s))\n",
        "\n",
        "      s = sizes[i]\n",
        "      for ar in aspect_ratios[i]:\n",
        "        boxes.append((cx, cy, s * math.sqrt(ar), s / math.sqrt(ar)))\n",
        "        boxes.append((cx, cy, s * math.sqrt(ar), s * math.sqrt(ar)))\n",
        "  return torch.tensor(boxes)\n",
        "\n",
        "def boundary_to_center(boundary_coordinates): #xy_to_cxcy\n",
        "  # take xmax and ymax divide by 2\n",
        "  xmax_ymax = boundary_coordinates[:,2:]\n",
        "  # take xmin and ymin divide by 2\n",
        "  xmin_ymin = boundary_coordinates[:,:2]\n",
        "  # center xmin and ymin\n",
        "  center_xmin_ymin = (xmax_ymax + xmin_ymin) / 2\n",
        "  # center xmax and ymax\n",
        "  center_xmax_ymax = xmax_ymax - xmin_ymin\n",
        "\n",
        "  return torch.cat([center_xmin_ymin, center_xmax_ymax], 1)\n",
        "\n",
        "def center_to_boundary(center_coordinates): #cxcy_to_xy\n",
        "  # take xmin and ymin\n",
        "  center_xmin_ymin = center_coordinates[:,:2]\n",
        "  # take xmax and ymax\n",
        "  center_xmax_ymax = center_coordinates[:,2:]\n",
        "  # divide max by 2\n",
        "  center_maxes = center_xmax_ymax / 2\n",
        "  # boundary xmin and ymin\n",
        "  boundary_xmin_ymin = center_xmin_ymin - center_maxes\n",
        "  # boundary xmax and ymax\n",
        "  boundary_xmax_ymax = center_xmin_ymin + center_maxes\n",
        "\n",
        "  return torch.cat([boundary_xmin_ymin, boundary_xmax_ymax],1)\n",
        "\n",
        "def encode(center_coordinates, default_boxes): #cxcy_to_gcxgcy\n",
        "  # get default xmin and ymin\n",
        "  default_xmin_ymin = default_boxes[:,:2]\n",
        "  # get default xmax and ymax\n",
        "  default_xmax_ymax = default_boxes[:,2:]\n",
        "  # encoded xmin and ymin values\n",
        "  encoded_x = (center_coordinates[:,:2] - default_xmin_ymin) / (default_xmax_ymax / 10)\n",
        "  # encoded xmax and ymax values\n",
        "  encoded_y = torch.log(center_coordinates[:,2:] / default_xmax_ymax) * 5\n",
        "\n",
        "  return torch.cat([encoded_x, encoded_y], 1)\n",
        "\n",
        "def decode(encoded_coordinates, default_boxes): #gcxgcy_to_cxcy\n",
        "  # get center xmin and ymin values\n",
        "  center_xmin_ymin = encoded_coordinates[:,:2] * default_boxes[:,2:] / 10 + default_boxes[:,:2]\n",
        "  # get center xmax and ymax values\n",
        "  center_xmax_ymax = torch.exp(encoded_coordinates[:,2:] / 5) * default_boxes[:,2:]\n",
        "\n",
        "  return torch.cat([center_xmin_ymin, center_xmax_ymax], 1)\n",
        "\n",
        "def iou(bbox1, bbox2):\n",
        "  # find intersections\n",
        "  # print(bbox1[:,:2].unsqueeze(1))\n",
        "  # print(bbox2)\n",
        "  lower_bound = torch.max(bbox1[:,:2].unsqueeze(1), bbox2[:,:2].unsqueeze(0))\n",
        "  upper_bound = torch.min(bbox1[:,2:].unsqueeze(1), bbox2[:,2:].unsqueeze(0))\n",
        "  intersection_dim = torch.clamp(upper_bound - lower_bound, min=0)\n",
        "  intersection = intersection_dim[:,:,0] * intersection_dim[:,:,1]\n",
        "\n",
        "  # get area of each box \n",
        "  area1 = (bbox1[:,2] - bbox1[:,0]) * (bbox1[:,3] - bbox1[:,1])\n",
        "  area2 = (bbox2[:,2] - bbox2[:,0]) * (bbox2[:,3] - bbox2[:,1])\n",
        "\n",
        "  # get the union of the area\n",
        "  union = area1.unsqueeze(1) + area2.unsqueeze(0) - intersection\n",
        "  return intersection / union"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo_Qdv7d1N1i"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEo97vuA1y1u"
      },
      "source": [
        "<h1>SSD MODEL</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLDwSBy1n_X"
      },
      "source": [
        "def decimate(tensor, m):\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d, index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "    return tensor"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSbn3sgr19gq"
      },
      "source": [
        "  class L2Norm(nn.Module):\n",
        "    def __init__(self, scale):\n",
        "        super(L2Norm, self).__init__()\n",
        "        self.scale = scale\n",
        "    \n",
        "    def forward(self, x, dim=1):\n",
        "        return self.scale * x * x.pow(2).sum(dim).clamp(min=1e-12).rsqrt().expand_as(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmqg0TUv2Cik"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        # Create standard VGG conv blocks 1-4\n",
        "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Apply L2 Normalization so that the num_features of 4_3 matches the num_features of other output layers (improves performance)\n",
        "        self.layer4_norm = L2Norm(20)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # Represent the fully connected layers FC6 & FC7 as convolutional layers to save computation time\n",
        "        # Dilation is used to downsize the total number of features in the layer to reduce computation\n",
        "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6)\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
        "        \n",
        "        self.Pretrain_VGG()\n",
        "        \n",
        "    def forward(self, xs):\n",
        "        x = F.relu(self.conv1_1(xs))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.max_pool1(x)\n",
        "        \n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.max_pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.max_pool3(x)\n",
        "\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        conv4_3_output = x # return conv4_3 output normalized\n",
        "        x = self.max_pool4(x)\n",
        "\n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = self.max_pool5(x)\n",
        "\n",
        "        x = F.relu(self.conv6(x))\n",
        "\n",
        "        x = F.relu(self.conv7(x))\n",
        "        conv7_output = x # return output of the final layer\n",
        "        \n",
        "        return conv4_3_output, conv7_output\n",
        "        \n",
        "    # Function for applying pretrained weights to VGG base for better performance/reduction in training time    \n",
        "    def Pretrain_VGG(self):\n",
        "        state_dict = self.state_dict()\n",
        "        keys = list(state_dict.keys())\n",
        "        pretrained_states = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_keys = list(pretrained_states.keys())\n",
        "        \n",
        "        for i, k in enumerate(keys[:-4]):\n",
        "            state_dict[k] = pretrained_states[pretrained_keys[i]]\n",
        "        \n",
        "        # Must perform decimation since originally layers 6 & 7 were fully connected, not conv2d\n",
        "        conv6_weights = pretrained_states['classifier.0.weight'].view(4096, 512, 7, 7)\n",
        "        conv6_bias = pretrained_states['classifier.0.bias']\n",
        "        state_dict['conv6.weight'] = decimate(conv6_weights, m=[4, None, 3, 3])\n",
        "        state_dict['conv6.bias'] = decimate(conv6_bias, m=[4])\n",
        "        \n",
        "        conv7_weights = pretrained_states['classifier.3.weight'].view(4096, 4096, 1, 1)\n",
        "        conv7_bias = pretrained_states['classifier.3.bias']\n",
        "        state_dict['conv7.weight'] = decimate(conv7_weights, m=[4, 4, None, None])\n",
        "        state_dict['conv7.bias'] = decimate(conv7_bias, m=[4])\n",
        "        \n",
        "        self.load_state_dict(state_dict)\n",
        "        \n",
        "        print(\"Successfully loaded pretrained weights\\n\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhXxTTSd2EdS"
      },
      "source": [
        "class AdditionalLayers(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdditionalLayers, self).__init__()\n",
        "        \n",
        "        # Stride 2 to reduce dimensionality of feature map\n",
        "        self.conv8_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, padding=0)\n",
        "        self.conv8_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2)\n",
        "        \n",
        "        self.conv9_1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv9_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2)\n",
        "        \n",
        "        self.conv10_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv10_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0)\n",
        "        \n",
        "        self.conv11_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv11_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0)\n",
        "        \n",
        "        self.xavier_init()\n",
        "    \n",
        "    # Authors recommend using the xavier method for initializing parameters\n",
        "    def xavier_init(self):\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "    \n",
        "    def forward(self, conv7_output):\n",
        "        x = F.relu(self.conv8_1(conv7_output))\n",
        "        x = F.relu(self.conv8_2(x))\n",
        "        conv8_2_output = x\n",
        "        \n",
        "        x = F.relu(self.conv9_1(x))\n",
        "        x = F.relu(self.conv9_2(x))\n",
        "        conv9_2_output = x\n",
        "        \n",
        "        x = F.relu(self.conv10_1(x))\n",
        "        x = F.relu(self.conv10_2(x))\n",
        "        conv10_2_output = x\n",
        "        \n",
        "        x = F.relu(self.conv11_1(x))\n",
        "        x = F.relu(self.conv11_2(x))\n",
        "        conv11_2_output = x\n",
        "        \n",
        "        return conv8_2_output, conv9_2_output, conv10_2_output, conv11_2_output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4rCG-Y2Ij0"
      },
      "source": [
        "class PredictionLayer(nn.Module):\n",
        "    classes = 21 # 20 classes + 1 class for 'no object'\n",
        "    num_boxes = [4, 6, 6, 6, 4, 4] # number of default boxes considered in each outputted feature map\n",
        "    map_dims = [512, 1024, 512, 256, 256, 256] # dimensionality of the in_channels for the layer\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(PredictionLayer, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Loc_layers: Layers that will output the location of predicted bounding boxes\n",
        "        # In_channels: The out_channels of feature map layer is convolving\n",
        "        # Out_channels: # of default boxes predicted * 4 (gcx, gcy, gw, gh) aka the offsets of the default box\n",
        "        self.loc_4_3 = nn.Conv2d(in_channels=512, out_channels=4 * 4, kernel_size=3, padding=1)\n",
        "        self.loc_7 = nn.Conv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1)\n",
        "        self.loc_8_2 = nn.Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1)\n",
        "        self.loc_9_2 = nn.Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1)\n",
        "        self.loc_10_2 = nn.Conv2d(in_channels=256, out_channels=4 * 4, kernel_size=3, padding=1)\n",
        "        self.loc_11_2 = nn.Conv2d(in_channels=256, out_channels=4 * 4, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Conf_layers: Layers that will output the confidence scores w.r.t each object class in the dataset\n",
        "        # for the object inside the predicted bounding box\n",
        "        # In_channels: The out_channels of feature map layer is convolving\n",
        "        # Out_channels: # of default boxes predicted * number of classes in the dataset + 1 extra class to denote background\n",
        "        # aka no object\n",
        "        self.conf_4_3 = nn.Conv2d(in_channels=512, out_channels=4 * num_classes, kernel_size=3, padding=1)\n",
        "        self.conf_7 = nn.Conv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1)\n",
        "        self.conf_8_2 = nn.Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1)\n",
        "        self.conf_9_2 = nn.Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1)\n",
        "        self.conf_10_2 = nn.Conv2d(in_channels=256, out_channels=4 * num_classes, kernel_size=3, padding=1)\n",
        "        self.conf_11_2 = nn.Conv2d(in_channels=256, out_channels=4 * num_classes, kernel_size=3, padding=1)\n",
        "            \n",
        "        self.xavier_init()\n",
        "        \n",
        "    def xavier_init(self):\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "    \n",
        "    def forward(self, conv4_3_output, conv7_output, conv8_2_output, conv9_2_output, conv10_2_output, conv11_2_output):\n",
        "        batch_size = conv4_3_output.size(0)\n",
        "        \n",
        "        loc_pred_4_3 = self.loc_4_3(conv4_3_output)\n",
        "        # Transform the output into a set of 5776 4-coordinate values\n",
        "        loc_pred_4_3 = loc_pred_4_3.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_4_3 = loc_pred_4_3.view(batch_size, -1, 4)\n",
        "        \n",
        "        loc_pred_7 = self.loc_7(conv7_output)\n",
        "        loc_pred_7 = loc_pred_7.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_7 = loc_pred_7.view(batch_size, -1, 4)\n",
        "        \n",
        "        loc_pred_8_2 = self.loc_8_2(conv8_2_output)\n",
        "        loc_pred_8_2 = loc_pred_8_2.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_8_2 = loc_pred_8_2.view(batch_size, -1, 4)\n",
        "        \n",
        "        loc_pred_9_2 = self.loc_9_2(conv9_2_output)\n",
        "        loc_pred_9_2 = loc_pred_9_2.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_9_2 = loc_pred_9_2.view(batch_size, -1, 4)\n",
        "        \n",
        "        loc_pred_10_2 = self.loc_10_2(conv10_2_output)\n",
        "        loc_pred_10_2 = loc_pred_10_2.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_10_2 = loc_pred_10_2.view(batch_size, -1, 4)\n",
        "        \n",
        "        loc_pred_11_2 = self.loc_11_2(conv11_2_output)\n",
        "        loc_pred_11_2 = loc_pred_11_2.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_pred_11_2 = loc_pred_11_2.view(batch_size, -1, 4)\n",
        "        \n",
        "        # Stack the predictions into a single-dimension 8732 x 4 tensor\n",
        "        loc_preds = torch.cat([loc_pred_4_3, loc_pred_7, loc_pred_8_2, loc_pred_9_2, loc_pred_10_2, loc_pred_11_2], dim=1)\n",
        "        \n",
        "        # Class confidence scores\n",
        "        conf_pred_4_3 = self.conf_4_3(conv4_3_output)\n",
        "        conf_pred_4_3 = conf_pred_4_3.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_4_3 = conf_pred_4_3.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_pred_7 = self.conf_7(conv7_output)\n",
        "        conf_pred_7 = conf_pred_7.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_7 = conf_pred_7.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_pred_8_2 = self.conf_8_2(conv8_2_output)\n",
        "        conf_pred_8_2 = conf_pred_8_2.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_8_2 = conf_pred_8_2.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_pred_9_2 = self.conf_9_2(conv9_2_output)\n",
        "        conf_pred_9_2 = conf_pred_9_2.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_9_2 = conf_pred_9_2.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_pred_10_2 = self.conf_10_2(conv10_2_output)\n",
        "        conf_pred_10_2 = conf_pred_10_2.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_10_2 = conf_pred_10_2.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_pred_11_2 = self.conf_11_2(conv11_2_output)\n",
        "        conf_pred_11_2 = conf_pred_11_2.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_pred_11_2 = conf_pred_11_2.view(batch_size, -1, self.num_classes)\n",
        "        \n",
        "        conf_preds = torch.cat([conf_pred_4_3, conf_pred_7, conf_pred_8_2, conf_pred_9_2, conf_pred_10_2, conf_pred_11_2], dim=1)\n",
        "        \n",
        "        return loc_preds, conf_preds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6geCWdDs2MsN"
      },
      "source": [
        "class SSD(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(SSD, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.vgg_base = VGG()\n",
        "        self.add_layers = AdditionalLayers()\n",
        "        self.pred_layer = PredictionLayer(self.num_classes)\n",
        "        \n",
        "        self.default_boxes = Create_Default_Boxes()\n",
        "\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "    def forward(self, img):\n",
        "        conv4_3_output, conv7_output = self.vgg_base(img)\n",
        "        norm = conv4_3_output.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
        "        conv4_3_output = conv4_3_output / norm\n",
        "        conv8_2_output, conv9_2_output, conv10_2_output, conv11_2_output = self.add_layers(conv7_output)\n",
        "        \n",
        "        loc_preds, conf_preds = self.pred_layer(conv4_3_output, conv7_output, conv8_2_output, conv9_2_output, conv10_2_output, conv11_2_output)\n",
        "        \n",
        "        return loc_preds, conf_preds"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLeZego2OeZ"
      },
      "source": [
        "<h1>MULTIBOX LOSS</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eatZduJA2Nkz"
      },
      "source": [
        "def Create_Default_Boxes():\n",
        "    default_boxes = []\n",
        "    \n",
        "    # Store the dimensionality, box scale, and aspect ratios of each convolution layer for easy access\n",
        "    feature_map_dim = {\n",
        "        '4_3': 38,\n",
        "        '7': 19,\n",
        "        '8_2': 10,\n",
        "        '9_2': 5,\n",
        "        '10_2': 3,\n",
        "        '11_2': 1\n",
        "    }\n",
        "    \n",
        "    scale = {\n",
        "        '4_3': 0.1,\n",
        "        '7': 0.2,\n",
        "        '8_2': 0.375,\n",
        "        '9_2': 0.55,\n",
        "        '10_2': 0.725,\n",
        "        '11_2': 0.9\n",
        "    }\n",
        "    \n",
        "    aspect_ratios = {\n",
        "        '4_3': [1., 2., 0.5],\n",
        "        '7': [1., 2., 3., 0.5, 0.333],\n",
        "        '8_2': [1., 2., 3., 0.5, 0.333],\n",
        "        '9_2': [1., 2., 3., 0.5, 0.333],\n",
        "        '10_2': [1., 2., 0.5],\n",
        "        '11_2': [1., 2., 0.5]\n",
        "    }\n",
        "    \n",
        "    map_names = list(feature_map_dim.keys())\n",
        "    \n",
        "    for k, feature_map in enumerate(map_names):\n",
        "        for i in range(feature_map_dim[feature_map]):\n",
        "            for j in range(feature_map_dim[feature_map]):\n",
        "                \n",
        "                # The center of each box will be at the center (.5, .5) of the x and y indices\n",
        "                cx = (j + 0.5) / feature_map_dim[feature_map]\n",
        "                cy = (i + 0.5) / feature_map_dim[feature_map]\n",
        "                \n",
        "                # Since w * h = s^2, and w/h = aspect_ratio:\n",
        "                # We can denote w as (w = scale * sqrt(aspect_ratio))\n",
        "                # We can denote h as (h = s / sqrt(aspect_ratio))\n",
        "                # Append to the box: [cx, cy, w (s * sqrt(a_r)), h (s / sqrt(a_r))]\n",
        "                for ar in aspect_ratios[feature_map]:\n",
        "                    default_boxes.append([cx, cy, scale[feature_map] * math.sqrt(ar), scale[feature_map] / math.sqrt(ar)])\n",
        "                    \n",
        "                    # Every feature map includes one additional default_box that has an aspect ratio of 1\n",
        "                    # This additional box uses a scale of the geometric mean between it's scale and the scale of the \n",
        "                    # next feature map over (k + 1)\n",
        "                    if ar == 1.:\n",
        "                        # We have to catch the obvious out of bounds we will get with the last index\n",
        "                        # In the case of the final feature map (11_2), we'll just do a geometric_mean of 1.\n",
        "                        try:\n",
        "                            geometric_mean = math.sqrt(scale[feature_map] * scale[map_names[k + 1]])\n",
        "                        except IndexError:\n",
        "                            geometric_mean = 1.\n",
        "                        \n",
        "                        default_boxes.append([cx, cy, geometric_mean, geometric_mean])\n",
        "    \n",
        "    default_boxes = torch.FloatTensor(default_boxes).to(device)\n",
        "    default_boxes.clamp(0, 1)\n",
        "    \n",
        "    return default_boxes\n",
        "\n",
        "def check_predictions(self, loc_preds, conf_preds, min_score, overlap_threshold, keep_cutoff, num_classes):\n",
        "    batch_size = loc_preds.size(0)\n",
        "    default_boxes = Create_Default_Boxes()\n",
        "    num_default = default_boxes.size(0)\n",
        "    \n",
        "    conf_scores = F.softmax(conf_preds, dim=2)\n",
        "    \n",
        "    # The kept boxes, labels, and scores for all of the images\n",
        "    final_boxes = list()\n",
        "    final_labels = list()\n",
        "    final_scores = list()\n",
        "    \n",
        "    assert num_default == loc_preds.size(1) == conf_preds.size(1)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        \n",
        "        # The kept boxes, labels, and scores for a single image\n",
        "        single_boxes = list()\n",
        "        single_labels = list()\n",
        "        single_scores = list()\n",
        "        \n",
        "        # Convert all of the predicted bounding boxes offsets of this image into xy style coordinates\n",
        "        converted_locs = center_to_boundary(decode(loc_preds[i], default_boxes))        \n",
        "        \n",
        "        for c in range(1, num_classes):\n",
        "            # get the confidence score for this class of all pred.bounding boxes of this image\n",
        "            class_scores = conf_scores[i][:, c]\n",
        "            # returns us a uint8 torch tensor that tells us if the class score > threshold\n",
        "            boxes_above_threshold = class_scores > min_score \n",
        "            \n",
        "            # check if any of the pred.bounding boxes had a class score above the threshold\n",
        "            num_above_threshold = boxes_above_threshold.sum().item()\n",
        "            if num_above_threshold == 0:\n",
        "                continue\n",
        "            class_scores = class_scores[boxes_above_threshold]\n",
        "            # get the locations of each bounding box with class score > threshold\n",
        "            box_locs = converted_locs[boxes_above_threshold]\n",
        "            \n",
        "            # sort boxes and respective locations by highest conf score\n",
        "            class_scores, idx = class_scores.sort(dim=0, descending=True)\n",
        "            box_locs = box_locs[idx]\n",
        "            \n",
        "            # We want to check the intersection of union between every box to remove boxes that cover essentially the same thing\n",
        "            ious = iou(box_locs, box_locs)\n",
        "            \n",
        "            # Perform non-maximum suppresion\n",
        "            nms = torch.zeros((num_above_threshold), dtype=torch.uint8).to(device)\n",
        "            \n",
        "            for box in range(box_locs.size(0)):\n",
        "                # Disregard suppressed boxes \n",
        "                if nms[box] == 1:\n",
        "                    continue\n",
        "                # Check the overlap of this box to every other box\n",
        "                # If iou > overlap then nms at that box index will = 1\n",
        "                # Otherwise it will remain unsuppressed\n",
        "                nms = torch.max(nms, ious[box] > overlap_threshold)\n",
        "                \n",
        "                # Note that the box will have overlap of 1 with itself\n",
        "                # Manually unsuppress box\n",
        "                nms[box] = 0\n",
        "            \n",
        "            # Store the unsuppressed boxes for the class\n",
        "            single_boxes.append(box_locs[1 - nms])\n",
        "            single_labels.append(torch.LongTensor((1 - nms).sum().item() * [c]).to(device))\n",
        "            single_scores.append(class_scores[1 - nms])\n",
        "            \n",
        "        if len(single_boxes) == 0:\n",
        "            single_boxes.append(torch.FloatTensor([0., 0., 1., 1.]).to(device))\n",
        "            single_labels.append(torch.LongTensor([0]).to(device))\n",
        "            single_scores.append(torch.FloatTensor[0.].to(device))\n",
        "        \n",
        "        # Create tensor out of lists\n",
        "        single_boxes = torch.cat(single_boxes, dim=0)\n",
        "        single_labels = torch.cat(single_labels, dim=0)\n",
        "        single_scores = torch.cat(single_scores, dim=0)\n",
        "        \n",
        "        # Keep the objects with the best scores\n",
        "        num_objects = single_boxes.size(0)\n",
        "        if num_objects > keep_cutoff:\n",
        "            # Sort by best scores\n",
        "            single_scores, idx = single_scores.sort(dim=0, descending=True)\n",
        "            # Remove the elements after the cutoff\n",
        "            single_scores = single_scores[:keep_cutoff]\n",
        "            single_boxes = single_boxes[idx][:keep_cutoff]\n",
        "            single_labels = single_labels[idx][:keep_cutoff]\n",
        "            \n",
        "        final_boxes.append(single_boxes)\n",
        "        final_labels.append(single_labels)\n",
        "        final_scores.append(single_scores)\n",
        "    \n",
        "    return final_boxes, final_labels, final_scores                   "
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fPqbcLh2kGd"
      },
      "source": [
        "class MultiboxLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes, default_boxes, threshold=0.5, hnm_ratio=3):\n",
        "        super(MultiboxLoss, self).__init__()\n",
        "        \n",
        "        self.smoothL1 = nn.L1Loss()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
        "        \n",
        "        self.default_boxes = default_boxes\n",
        "        self.converted_defaults = center_to_boundary(default_boxes)\n",
        "        self.num_classes = num_classes\n",
        "        self.hnm_ratio = hnm_ratio\n",
        "        self.threshold = threshold\n",
        "        self.alpha = 1\n",
        "    \n",
        "    def forward(self, loc_preds, conf_preds, ground_truths, labels):\n",
        "        batch_size = loc_preds.size(0)\n",
        "        num_default = self.default_boxes.size(0)\n",
        "        num_classes = conf_preds.size(2)\n",
        "        \n",
        "        assert num_default == loc_preds.size(1) == conf_preds.size(1)\n",
        "        \n",
        "        ground_truth_locs = torch.zeros((batch_size, num_default, 4), dtype=torch.float).to(device)\n",
        "        ground_truth_classes = torch.zeros((batch_size, num_default), dtype=torch.long).to(device)\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            # get all of the ground truth bounding boxes from image annotations\n",
        "            num_objects = ground_truths[i].size(0)\n",
        "            \n",
        "            # We want to match all of the ground truth bounding boxes to a default box\n",
        "            # This is just in case a ground truth object was not matched with a predicted bounding box\n",
        "            \n",
        "            # Find the jaccard overlap between the ground truth objects' bounding box and all default boxes\n",
        "            ious = iou(ground_truths[i], self.default_boxes)\n",
        "            \n",
        "            # For each default box, get the ground truth object that overlaps with that box the most\n",
        "            best_overlap_default, corresponding_object = ious.max(dim=0)\n",
        "            \n",
        "            # For each object, find the default box with the max overlap\n",
        "            _, best_overlap_object = ious.max(dim=1)\n",
        "            \n",
        "            # Assign each object to the best prior\n",
        "            corresponding_object[best_overlap_object] = torch.LongTensor(range(num_objects)).to(device)\n",
        "            \n",
        "            # Set the overlap value for each object's best default box with said object greater than the threshold\n",
        "            best_overlap_default[best_overlap_object] = 1.\n",
        "            \n",
        "            # Apply labels to each default box where said label has an overlap better than the threshold\n",
        "            \n",
        "            default_box_labels = labels[i][corresponding_object]\n",
        "            default_box_labels[best_overlap_default < self.threshold] = 0\n",
        "            \n",
        "            ground_truth_classes[i] = default_box_labels\n",
        "            \n",
        "            ground_truth_locs[i] = encode(boundary_to_center(ground_truths[i][corresponding_object]), self.default_boxes)\n",
        "            \n",
        "        positive_boxes = ground_truth_classes != 0\n",
        "        \n",
        "        loc_loss = self.smoothL1(loc_preds[positive_boxes], ground_truth_locs[positive_boxes])\n",
        "        \n",
        "        num_positives = positive_boxes.sum(dim=1)\n",
        "        num_negatives = self.hnm_ratio * num_positives\n",
        "        \n",
        "        conf_loss_all = self.cross_entropy(conf_preds.view(-1, num_classes), ground_truth_classes.view(-1))\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, num_default)\n",
        "        \n",
        "        conf_loss_pos = conf_loss_all[positive_boxes]\n",
        "        \n",
        "        conf_loss_neg = conf_loss_all.clone()\n",
        "        conf_loss_neg[positive_boxes] = 0.\n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
        "        hardness_ranks = torch.LongTensor(range(num_default)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
        "        negatives = hardness_ranks < num_negatives.unsqueeze(1)\n",
        "        conf_loss_hard_neg = conf_loss_neg[negatives]\n",
        "        \n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / num_positives.sum().float()\n",
        "        \n",
        "        return conf_loss + self.alpha * loc_loss"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOCIGGiu3TOt"
      },
      "source": [
        "<h1>TRAINING</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyng6bSf3Qmg"
      },
      "source": [
        "# Setting training parameters\n",
        "batch_size = 8\n",
        "iterations = 120000\n",
        "print_freq = 200\n",
        "lr = 1e-3\n",
        "decay_lr_point = [80000, 100000]\n",
        "decay_lr_to = 0.1\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "grad_clip = None\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4RBXrH95gHz"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ge52zS5Me7"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  batch_time = AverageMeter()\n",
        "  data_time = AverageMeter()\n",
        "  losses = AverageMeter()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
        "      data_time.update(time.time() - start)\n",
        "\n",
        "      # Move to default device\n",
        "      images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
        "      boxes = [b.to(device) for b in boxes]\n",
        "      labels = [l.to(device) for l in labels]\n",
        "\n",
        "      # Forward prop.\n",
        "      predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "      # Loss\n",
        "      loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "\n",
        "      # Backward prop.\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip gradients, if necessary\n",
        "      if grad_clip is not None:\n",
        "          clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "      # Update model\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "      batch_time.update(time.time() - start)\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      # Print status\n",
        "      if i % print_freq == 0:\n",
        "          print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
        "                                                                batch_time=batch_time,\n",
        "                                                                data_time=data_time, loss=losses))\n",
        "  del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr78ZHWr5-V5"
      },
      "source": [
        "n_classes = len(labels_map)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "start_epoch = 0"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "HKfoTwtL6gIf",
        "outputId": "12385552-fe9c-495f-f34c-d0357a763685"
      },
      "source": [
        "model = SSD(num_classes=n_classes)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded pretrained weights\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-189025472382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-0aed7e09f374>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreate_Default_Boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-c8fdc2d1a4ed>\u001b[0m in \u001b[0;36mCreate_Default_Boxes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0mdefault_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeometric_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeometric_mean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mdefault_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mdefault_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLsg2Sg862TR"
      },
      "source": [
        "biases = list()\n",
        "not_biases = list()\n",
        "\n",
        "for param_name, param in model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    if param_name.endswith('.bias'):\n",
        "      biases.append(param)\n",
        "    else:\n",
        "      not_biases.append(param)\n",
        "\n",
        "optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}], lr=lr, momentum=momentum, weight_decay=weight_decay)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "00IUIvga7eIW",
        "outputId": "aefee841-47cb-4f71-dee3-1e337a46b41e"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = MultiboxLoss(default_boxes=model.default_boxes, num_classes=num_classes).to(device)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-995af379639b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiboxLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psblQa1rMh7I",
        "outputId": "4bf7131c-6532-4857-aa50-12ea3bdf5aee"
      },
      "source": [
        "print(model.default_boxes)\n",
        "print(criterion.default_boxes)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0132, 0.0132, 0.1000, 0.1000],\n",
            "        [0.0132, 0.0132, 0.1414, 0.1414],\n",
            "        [0.0132, 0.0132, 0.1414, 0.0707],\n",
            "        ...,\n",
            "        [0.5000, 0.5000, 1.0000, 1.0000],\n",
            "        [0.5000, 0.5000, 1.2728, 0.6364],\n",
            "        [0.5000, 0.5000, 0.6364, 1.2728]], device='cuda:0')\n",
            "tensor([[0.0132, 0.0132, 0.1000, 0.1000],\n",
            "        [0.0132, 0.0132, 0.1414, 0.1414],\n",
            "        [0.0132, 0.0132, 0.1414, 0.0707],\n",
            "        ...,\n",
            "        [0.5000, 0.5000, 1.0000, 1.0000],\n",
            "        [0.5000, 0.5000, 1.2728, 0.6364],\n",
            "        [0.5000, 0.5000, 0.6364, 1.2728]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLOqwlC_McE"
      },
      "source": [
        "def collate_fn(batch):\n",
        "        \"\"\"\n",
        "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
        "        This describes how to combine these tensors of different sizes. We use lists.\n",
        "        Note: this need not be defined in this Class, can be standalone.\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "        difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            boxes.append(b[3])\n",
        "            labels.append(b[2])\n",
        "            difficulties.append(b[4])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqw4ot_k7qVU"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "epochs = iterations // (len(train_dataset) // 32)\n",
        "\n",
        "decay_lr_point = [it // (len(train_dataset) // 32) for it in decay_lr_point]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "84grWM-V8K0r",
        "outputId": "7002e0d7-f67a-42b8-84b4-733675826bec"
      },
      "source": [
        "for epoch in range(start_epoch, epochs):\n",
        "  if epoch in decay_lr_point:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * decay_lr_to\n",
        "  \n",
        "  train(train_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, epoch=epoch)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/866]\tBatch Time 12.972 (12.972)\tData Time 0.003 (0.003)\tLoss 20.9112 (20.9112)\t\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-eb65e1e24d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecay_lr_to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-75afcc2a90bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;31m# Backward prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Clip gradients, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ1nz0fYBSxQ"
      },
      "source": [
        "train_dataset[0][2]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}